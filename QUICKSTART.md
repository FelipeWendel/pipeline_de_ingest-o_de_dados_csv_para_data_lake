# ğŸš€ Guia RÃ¡pido de InÃ­cio

Comece a usar o Pipeline de IngestÃ£o de Dados CSV em minutos!

## âš¡ InÃ­cio RÃ¡pido (5 minutos)

### 1. PrÃ©-requisitos

```bash
# Verificar instalaÃ§Ãµes
python --version        # Python 3.9+
terraform --version     # Terraform 1.0+
aws --version          # AWS CLI 2.x

# Verificar credenciais AWS
aws sts get-caller-identity
```

### 2. ConfiguraÃ§Ã£o Inicial

```powershell
# No Windows PowerShell

# Clonar/navegar para o projeto
cd pipeline_de_ingestÃ£o_de_dados_csv_para_data_lake

# Configurar ambiente virtual
python -m venv venv
.\venv\Scripts\Activate.ps1

# Instalar dependÃªncias
pip install -r requirements.txt

# Copiar arquivo de configuraÃ§Ã£o
Copy-Item .env.example .env

# Editar .env com suas configuraÃ§Ãµes
notepad .env
```

**Edite o arquivo `.env`:**
```bash
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=SEU_ACCOUNT_ID
RAW_BUCKET_NAME=seu-raw-bucket-unico-123456
DATA_LAKE_BUCKET_NAME=seu-datalake-bucket-unico-123456
```

### 3. Deploy da Infraestrutura

```powershell
# Navegar para terraform
cd terraform

# Copiar e editar variÃ¡veis
Copy-Item terraform.tfvars.example terraform.tfvars
notepad terraform.tfvars

# Deploy
terraform init
terraform plan
terraform apply
# Digite 'yes' para confirmar
```

â±ï¸ **Aguarde 3-5 minutos** para o deploy completar.

### 4. Teste o Pipeline

```powershell
# Voltar para raiz do projeto
cd ..

# Upload de arquivo de teste
aws s3 cp data\sample\test_data.csv s3://seu-raw-bucket/input/

# Monitorar logs (em nova janela)
aws logs tail /aws/lambda/csv-data-lake-pipeline-csv-ingestor --follow

# Verificar resultado
aws s3 ls s3://seu-datalake-bucket/processed/ --recursive
```

ğŸ‰ **Pronto!** Seu pipeline estÃ¡ funcionando!

---

## ğŸ“š PrÃ³ximos Passos

### Consultar Dados com Athena

1. Acesse o [AWS Athena Console](https://console.aws.amazon.com/athena/)
2. Execute o Glue Crawler:
   ```powershell
   aws glue start-crawler --name csv-data-lake-pipeline-crawler
   ```
3. Aguarde o crawler finalizar (~2 minutos)
4. No Athena, selecione o database: `csv_data_lake_pipeline_db`
5. Execute query:
   ```sql
   SELECT * FROM sua_tabela LIMIT 10;
   ```

### Monitoramento

```powershell
# Ver dashboard
echo "https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:"

# Ver logs em tempo real
aws logs tail /aws/lambda/csv-data-lake-pipeline-csv-ingestor --follow

# Verificar mÃ©tricas
aws cloudwatch get-metric-statistics `
  --namespace AWS/Lambda `
  --metric-name Invocations `
  --dimensions Name=FunctionName,Value=csv-data-lake-pipeline-csv-ingestor `
  --start-time 2024-01-01T00:00:00Z `
  --end-time 2024-12-31T23:59:59Z `
  --period 3600 `
  --statistics Sum
```

---

## ğŸ› ï¸ Comandos Ãšteis

### Usando scripts.ps1 (Windows)

```powershell
# Ver todos os comandos
.\scripts.ps1 help

# Executar testes
.\scripts.ps1 test

# Limpar projeto
.\scripts.ps1 clean

# Upload de CSV
.\scripts.ps1 upload-csv seu-raw-bucket

# Listar Data Lake
.\scripts.ps1 list-datalake seu-datalake-bucket

# Ver logs
.\scripts.ps1 logs csv-data-lake-pipeline-csv-ingestor
```

### Terraform

```powershell
# Ver outputs
cd terraform
terraform output

# Ver estado
terraform show

# Atualizar infraestrutura
terraform apply

# Destruir tudo
terraform destroy
```

---

## ğŸ“ Estrutura do Projeto

```
pipeline_de_ingestÃ£o_de_dados_csv_para_data_lake/
â”œâ”€â”€ src/                          # CÃ³digo fonte
â”‚   â”œâ”€â”€ config/                  # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ ingestion/               # LÃ³gica de ingestÃ£o
â”‚   â”‚   â”œâ”€â”€ csv_processor.py    # Processador CSV
â”‚   â”‚   â””â”€â”€ pipeline.py         # Orquestrador
â”‚   â”œâ”€â”€ lambda_functions/        # FunÃ§Ãµes Lambda
â”‚   â”‚   â””â”€â”€ csv_ingestor.py     # Handler Lambda
â”‚   â””â”€â”€ utils/                   # UtilitÃ¡rios
â”‚       â”œâ”€â”€ s3_utils.py         # Cliente S3
â”‚       â””â”€â”€ logger.py           # Logging
â”œâ”€â”€ terraform/                    # Infraestrutura
â”‚   â”œâ”€â”€ main.tf                 # ConfiguraÃ§Ã£o principal
â”‚   â”œâ”€â”€ s3.tf                   # Buckets S3
â”‚   â”œâ”€â”€ lambda.tf               # Lambda functions
â”‚   â”œâ”€â”€ glue.tf                 # Glue catalog
â”‚   â””â”€â”€ monitoring.tf           # CloudWatch
â”œâ”€â”€ tests/                        # Testes
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ architecture.md         # Arquitetura
â”‚   â””â”€â”€ deployment.md           # Guia de deploy
â”œâ”€â”€ data/sample/                  # Dados de exemplo
â”œâ”€â”€ README.md                     # DocumentaÃ§Ã£o principal
â””â”€â”€ requirements.txt              # DependÃªncias Python
```

---

## ğŸ”§ Troubleshooting RÃ¡pido

### Erro: Bucket jÃ¡ existe
```
Error: BucketAlreadyExists
```
**SoluÃ§Ã£o**: Escolha nomes Ãºnicos em `terraform.tfvars`

### Lambda timeout
```
Task timed out after 300.00 seconds
```
**SoluÃ§Ã£o**: Aumente `lambda_timeout` em `terraform.tfvars`

### PermissÃ£o negada
```
AccessDenied
```
**SoluÃ§Ã£o**: Verifique IAM roles e AWS credentials
```powershell
aws sts get-caller-identity
aws iam list-roles
```

### Logs nÃ£o aparecem
**SoluÃ§Ã£o**: Aguarde 10-30 segundos. Logs podem demorar.

### Crawler nÃ£o encontra dados
**SoluÃ§Ã£o**: 
1. Verifique se Lambda processou o arquivo
2. Execute crawler manualmente
3. Verifique path no S3

---

## ğŸ“Š Workflow do Pipeline

```
1. Upload CSV â†’ S3 Raw Bucket (input/)
           â†“
2. S3 Event â†’ Trigger Lambda
           â†“
3. Lambda â†’ Processa CSV
           â†“
4. Lambda â†’ Salva Parquet no Data Lake (particionado)
           â†“
5. Glue Crawler â†’ Cataloga dados
           â†“
6. Athena/Redshift â†’ Consulta dados
```

---

## ğŸ¯ Casos de Uso

### 1. IngestÃ£o DiÃ¡ria de Vendas
```powershell
# Upload diÃ¡rio
aws s3 cp vendas_$(Get-Date -Format "yyyyMMdd").csv s3://seu-raw-bucket/input/
```

### 2. Processamento em Lote
```powershell
# Upload mÃºltiplos arquivos
aws s3 sync ./arquivos_csv/ s3://seu-raw-bucket/input/
```

### 3. IntegraÃ§Ã£o com BI
- Configure Athena como data source no Power BI/Tableau
- Use Glue Catalog como metastore
- Consulte dados particionados para performance

---

## ğŸ“– DocumentaÃ§Ã£o Completa

- [Arquitetura Detalhada](docs/architecture.md)
- [Guia de Deploy Completo](docs/deployment.md)
- [Como Contribuir](CONTRIBUTING.md)
- [Changelog](CHANGELOG.md)

---

## ğŸ†˜ Precisa de Ajuda?

1. Verifique os logs: `.\scripts.ps1 logs nome-da-funcao`
2. Consulte a [documentaÃ§Ã£o completa](docs/)
3. Abra uma [issue](../../issues)

---

## âœ… Checklist de ValidaÃ§Ã£o

- [ ] AWS CLI configurado e funcionando
- [ ] Terraform instalado (versÃ£o 1.0+)
- [ ] Python 3.9+ instalado
- [ ] Arquivo `.env` configurado
- [ ] Arquivo `terraform.tfvars` configurado
- [ ] Infraestrutura deployada com sucesso
- [ ] Teste de upload de CSV bem-sucedido
- [ ] Lambda processou o arquivo
- [ ] Arquivo Parquet no Data Lake
- [ ] Glue Crawler executado
- [ ] Dados visÃ­veis no Athena

---

**Bom uso do pipeline! ğŸš€**

Para dÃºvidas, consulte a [documentaÃ§Ã£o completa](docs/) ou abra uma issue.
