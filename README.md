# Pipeline de IngestÃ£o CSV para Data Lake AWS ğŸš€

Pipeline serverless que automatiza ingestÃ£o e transformaÃ§Ã£o de arquivos CSV em um Data Lake AWS.

## ğŸ—ï¸ Arquitetura

```
Upload CSV â†’ S3 Raw â†’ Lambda â†’ Processamento â†’ S3 Data Lake (Parquet) â†’ CatÃ¡logo Glue â†’ Athena
```

**Fluxo:**
1. Upload de CSV no S3 Raw Bucket
2. Acionamento automÃ¡tico da Lambda
3. Processamento: CSV â†’ Parquet + Limpeza + Particionamento
4. Armazenamento no Data Lake (particionado por data)
5. CatalogaÃ§Ã£o automÃ¡tica no AWS Glue

## ğŸ’» Stack TecnolÃ³gica

- **AWS Lambda** - Processamento serverless
- **Amazon S3** - Armazenamento (Raw + Data Lake)
- **AWS Glue** - CatÃ¡logo de Dados
- **Python 3.9** - pandas, pyarrow, boto3
- **Terraform** - Infraestrutura como CÃ³digo

## âœ¨ Funcionalidades

âœ… ConversÃ£o CSV â†’ Parquet (80% menos armazenamento)  
âœ… Particionamento por data (ano/mÃªs/dia)  
âœ… Limpeza automÃ¡tica de dados  
âœ… CatalogaÃ§Ã£o para consultas SQL (Athena)  
âœ… Logs no CloudWatch  
âœ… Alarmes de erro  

## ğŸ“ Estrutura (~550 linhas)

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/         # Pipeline e processador CSV (102 linhas)
â”‚   â”œâ”€â”€ lambda_functions/  # Handler Lambda (57 linhas)
â”‚   â”œâ”€â”€ utils/             # Cliente S3 (28 linhas)
â”‚   â””â”€â”€ config/            # ConfiguraÃ§Ãµes (74 linhas)
â”œâ”€â”€ terraform/             # Infraestrutura completa (365 linhas)
â”‚   â”œâ”€â”€ s3.tf             # Buckets
â”‚   â”œâ”€â”€ lambda.tf         # FunÃ§Ã£o Lambda
â”‚   â”œâ”€â”€ iam.tf            # FunÃ§Ãµes e polÃ­ticas
â”‚   â”œâ”€â”€ glue.tf           # CatÃ¡logo de Dados
â”‚   â””â”€â”€ monitoring.tf     # CloudWatch
â””â”€â”€ tests/                 # Testes unitÃ¡rios
```

## ğŸš€ ImplantaÃ§Ã£o RÃ¡pida

### 1. PrÃ©-requisitos
```bash
python --version  # Python 3.9+
terraform --version  # Terraform 1.0+
aws --version  # AWS CLI configurado
```

### 2. ConfiguraÃ§Ã£o
```bash
# Criar ambiente virtual
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows
source venv/bin/activate      # Linux/Mac

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 3. Configurar AWS
Edite `terraform/terraform.tfvars`:
```hcl
project_name         = "csv-pipeline"
aws_region          = "us-east-1"
raw_bucket_name     = "seu-raw-bucket-123"
data_lake_bucket    = "seu-datalake-bucket-123"
```

### 4. ImplantaÃ§Ã£o
```bash
cd terraform
terraform init
terraform apply
```

## ğŸ“Š Uso

```bash
# Upload CSV para processar
aws s3 cp arquivo.csv s3://seu-raw-bucket-123/

# Verificar resultado
aws s3 ls s3://seu-datalake-bucket-123/data/ --recursive

# Consulta com Athena
aws athena start-query-execution \
  --query-string "SELECT * FROM data_lake.csv_data LIMIT 10"
```

## ğŸ“ˆ Resultados

- ğŸ¯ **80% reduÃ§Ã£o** nos custos de armazenamento
- âš¡ **10x mais rÃ¡pido** - consultas em Parquet
- ğŸ”„ **100% automatizado** - zero intervenÃ§Ã£o manual
- ğŸ“Š **EscalÃ¡vel** - processa milhares de arquivos/dia

## ğŸ§ª Testes

```bash
pytest tests/ -v
```

## ğŸ“ Logs

```bash
# Ver logs da Lambda
aws logs tail /aws/lambda/csv-pipeline-csv-ingestor --follow
```

### Monitoramento

```bash
# Visualizar logs
aws logs tail /aws/lambda/csv-ingestor --follow

# Verificar status do pipeline
python src/utils/check_pipeline_status.py
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest tests/

# Executar testes especÃ­ficos
pytest tests/test_ingestion.py -v
```

## ğŸ“ Formato de Dados

### Entrada (CSV)
- Arquivos CSV com cabeÃ§alho
- CodificaÃ§Ã£o UTF-8
- Delimitador: vÃ­rgula (,)

### SaÃ­da (Parquet)
- Formato colunar Parquet
- Particionado por data (ano/mÃªs/dia)
- CompressÃ£o snappy

## ğŸ”’ SeguranÃ§a

- Buckets S3 com criptografia habilitada (SSE-S3)
- FunÃ§Ãµes IAM com princÃ­pio de menor privilÃ©gio
- Endpoints VPC para comunicaÃ§Ã£o privada
- CloudTrail habilitado para auditoria

## ğŸ“ˆ Monitoramento e Alertas

- Logs CloudWatch para todas as funÃ§Ãµes Lambda
- MÃ©tricas CloudWatch personalizadas
- SNS para notificaÃ§Ãµes de falhas
- X-Ray para rastreamento distribuÃ­do

## ğŸ¤ Contribuindo

1. FaÃ§a fork do projeto
2. Crie uma branch para sua funcionalidade (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas alteraÃ§Ãµes (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.

## ğŸ‘¤ Autor

Engenheiro de Dados - Pipeline de IngestÃ£o AWS

## ğŸ†˜ Suporte

Para dÃºvidas e suporte, abra uma issue no repositÃ³rio.
